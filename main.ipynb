{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import wfdb\r\n",
    "import ast\r\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\r\n",
    "import os\r\n",
    "\r\n",
    "from sklearn.utils import shuffle\r\n",
    "import math\r\n",
    "import tensorflow as tf\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.preprocessing import StandardScaler,normalize, MinMaxScaler\r\n",
    "\r\n",
    "\r\n",
    "import os\r\n",
    "import wandb\r\n",
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\r\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing \r\n",
    "#   Using the super classes, multi label classification, excluding samples with no labels and considering atleast one label\r\n",
    "\r\n",
    "path = 'ptb/'\r\n",
    "Y = pd.read_csv(path+ 'ptbxl_database.csv', index_col = 'ecg_id')\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "data = np.array([wfdb.rdsamp(path+f)[0] for f in Y.filename_lr])\r\n",
    "Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\r\n",
    "    \r\n",
    "agg_df = pd.read_csv(path+ 'scp_statements.csv', index_col = 0)\r\n",
    "\r\n",
    "agg_df = agg_df[agg_df.diagnostic == 1]\r\n",
    "\r\n",
    "def agg(y_dic):\r\n",
    "    temp =[]\r\n",
    "    \r\n",
    "    for key in y_dic.keys():\r\n",
    "        if key in agg_df.index:\r\n",
    "            c = agg_df.loc[key].diagnostic_class\r\n",
    "            if str(c) != 'nan':\r\n",
    "                temp.append(c)\r\n",
    "    return list(set(temp))\r\n",
    "\r\n",
    "Y['diagnostic_superclass'] = Y.scp_codes.apply(agg)\r\n",
    "Y['superdiagnostic_len'] = Y['diagnostic_superclass'].apply(lambda x: len(x))\r\n",
    "\r\n",
    "\r\n",
    "#########\r\n",
    "\r\n",
    "counts = pd.Series(np.concatenate(Y.diagnostic_superclass.values)).value_counts()\r\n",
    "\r\n",
    "Y['diagnostic_superclass'] = Y['diagnostic_superclass'].apply(lambda x: list(set(x).intersection(set(counts.index.values))))\r\n",
    "\r\n",
    "X_data = data[Y['superdiagnostic_len'] >= 1]\r\n",
    "Y_data = Y[Y['superdiagnostic_len'] >= 1]\r\n",
    "\r\n",
    "mlb = MultiLabelBinarizer()\r\n",
    "mlb.fit(Y_data['diagnostic_superclass'])\r\n",
    "y = mlb.transform(Y_data['diagnostic_superclass'].values)\r\n",
    "\r\n",
    "########\r\n",
    "\r\n",
    "## Stratify split\r\n",
    "\r\n",
    "X_train = X_data[Y_data.strat_fold < 9]\r\n",
    "y_train = y[Y_data.strat_fold < 9]\r\n",
    "\r\n",
    "X_val = X_data[Y_data.strat_fold == 9]\r\n",
    "y_val = y[Y_data.strat_fold == 9]\r\n",
    "\r\n",
    "X_test = X_data[Y_data.strat_fold == 10]\r\n",
    "y_test = y[Y_data.strat_fold == 10]\r\n",
    "\r\n",
    "del X_data, Y_data, y\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "\n",
    "# Standardizing\n",
    "\n",
    "def apply_scaler(X, scaler):\n",
    "    X_tmp = []\n",
    "    for x in X:\n",
    "        x_shape = x.shape\n",
    "        X_tmp.append(scaler.transform(x.flatten()[:,np.newaxis]).reshape(x_shape))\n",
    "    X_tmp = np.array(X_tmp)\n",
    "    return X_tmp\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(np.vstack(X_train).flatten()[:,np.newaxis].astype(float))\n",
    "\n",
    "X_train_scale = apply_scaler(X_train, scaler)\n",
    "X_test_scale = apply_scaler(X_test, scaler)\n",
    "X_val_scale = apply_scaler(X_val, scaler)\n",
    "\n",
    "del X_train, X_test, X_val\n",
    "\n",
    "## Shuffling\n",
    "\n",
    "X_train_scale, y_train = shuffle(X_train_scale, y_train, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y,batch_size = 16):\n",
    "        self.batch_size = batch_size\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.X) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        X_full = self.X[idx * self.batch_size:(idx + 1) *self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) *self.batch_size]\n",
    "\n",
    "            \n",
    "        return np.transpose(X_full[..., np.newaxis], (0, 2, 1, 3)) ,batch_y\n",
    "    \n",
    "## Params\n",
    "\n",
    "batch_size = 32\n",
    "    \n",
    "train_gen = DataGen(X_train_scale, y_train, batch_size = batch_size)\n",
    "test_gen = DataGen(X_test_scale, y_test, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 12, 1000, 1)\n"
     ]
    }
   ],
   "source": [
    "test = train_gen[0][0].shape\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\r\n",
    "\r\n",
    "class attention(tf.keras.layers.Layer):\r\n",
    "    \r\n",
    "    def __init__(self, return_sequences = False, dim = 32, **kwargs):\r\n",
    "        self.return_sequences = return_sequences\r\n",
    "        self.dim = dim\r\n",
    "        super(attention,self).__init__(**kwargs)\r\n",
    "        \r\n",
    "    def build(self, input_shape):\r\n",
    "        \r\n",
    "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1], self.dim),\r\n",
    "                               initializer=\"normal\")\r\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1], self.dim),\r\n",
    "                               initializer=\"zeros\")\r\n",
    "        self.V = self.add_weight(name = \"Vatt\", shape = (self.dim, 1), initializer = \"normal\")\r\n",
    "        \r\n",
    "        super(attention,self).build(input_shape)\r\n",
    "        \r\n",
    "    def call(self, x):\r\n",
    "        \r\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\r\n",
    "        e = K.dot(e, self.V)\r\n",
    "        a = K.softmax(e, axis=1)\r\n",
    "        output = x*a\r\n",
    "        \r\n",
    "        if self.return_sequences :\r\n",
    "            return output, a\r\n",
    "        \r\n",
    "        return K.sum(output, axis=1), a\r\n",
    "\r\n",
    "    def get_config(self):\r\n",
    "        base_config = super().get_config()\r\n",
    "        config = {\"return sequences\" : tf.keras.initializers.serialize(self.return_sequences), \"att dim\" : tf.keras.initializers.serialize(self.dim)}\r\n",
    "        return dict(list(base_config.items()) + list(config.items()))\r\n",
    "    \r\n",
    "## Resnet blocks\r\n",
    "\r\n",
    "def relu_bn(inputs: tf.Tensor) -> tf.Tensor:\r\n",
    "    \r\n",
    "    \r\n",
    "    dp = Dropout(0.5)(inputs)\r\n",
    "    relu = ReLU()(dp)\r\n",
    "    bn = BatchNormalization()(relu)\r\n",
    "    return bn\r\n",
    "\r\n",
    "\r\n",
    "def residual_block(x: tf.Tensor, downsample: bool, filters: int, kernel_size: int = 12) -> tf.Tensor:\r\n",
    "    \r\n",
    "    y = Conv1D(kernel_size=kernel_size,\r\n",
    "               strides= (1 if not downsample else 2),\r\n",
    "               filters=filters,\r\n",
    "               padding=\"same\")(x)\r\n",
    "    y = relu_bn(y)\r\n",
    "    y = Conv1D(kernel_size=kernel_size,\r\n",
    "               strides=1,\r\n",
    "               filters=filters,\r\n",
    "               padding=\"same\")(y)\r\n",
    "\r\n",
    "    if downsample:\r\n",
    "        x = Conv1D(kernel_size=1,\r\n",
    "                   strides=2,\r\n",
    "                   filters=filters,\r\n",
    "                   padding=\"same\")(x)\r\n",
    "    out = Add()([x, y])\r\n",
    "    out = relu_bn(out)\r\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Params\n",
    "\n",
    "sig_len = 1000\n",
    "beat_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 12, 1000, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 50, 1)]      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 50, 16)       144         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 50, 16)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 50, 16)       2064        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 50, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 50, 16)       64          re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 50, 16)       2064        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 50, 16)       0           activation[0][0]                 \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 50, 16)       0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 50, 16)       64          re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 50, 16)       2064        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 50, 16)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 50, 16)       64          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 50, 16)       2064        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 50, 16)       0           batch_normalization_1[0][0]      \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 50, 16)       0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 50, 16)       64          re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 25, 32)       4128        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 25, 32)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 25, 32)       128         re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 25, 32)       544         batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 25, 32)       8224        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 25, 32)       0           conv1d_7[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 25, 32)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 25, 32)       128         re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 25, 32)       8224        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 25, 32)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 25, 32)       128         re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 25, 32)       8224        batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 25, 32)       0           batch_normalization_5[0][0]      \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 25, 32)       0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 25, 32)       128         re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 13, 64)       16448       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 13, 64)       0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 13, 64)       256         re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 13, 64)       2112        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 13, 64)       32832       batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 13, 64)       0           conv1d_12[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 13, 64)       0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 13, 64)       256         re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 13, 64)       32832       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 13, 64)       0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 13, 64)       256         re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 13, 64)       32832       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 13, 64)       0           batch_normalization_9[0][0]      \n",
      "                                                                 conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 13, 64)       0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 13, 64)       256         re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "beat_att (attention)            ((None, 64), (None,  2496        batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 64)]     0           beat_att[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 20, 64)       24832       tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "rhythm_att (attention)          ((None, 64), (None,  2720        bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 12, 64)]     0           rhythm_att[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "channel_att (attention)         ((None, 64), (None,  2464        tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 5)            325         channel_att[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 189,429\n",
      "Trainable params: 188,533\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Input, Attention, LSTM, Activation, Dense, Average,ReLU, BatchNormalization,Add, Reshape, Bidirectional, Concatenate\r\n",
    "\r\n",
    "num_channel = 12\r\n",
    "num_filters = 32\r\n",
    "num_blocks_list = [2, 2, 2]\r\n",
    "\r\n",
    "inputs = Input(shape = (num_channel, sig_len, 1), batch_size = None)\r\n",
    "\r\n",
    "#### Beat Level \r\n",
    "x = K.reshape(inputs, (-1, beat_size,1 ))\r\n",
    "\r\n",
    "x = Conv1D(32 ,12 ,padding = 'same')(x)\r\n",
    "x = Activation('relu')(x)\r\n",
    "\r\n",
    "for i in range(len(num_blocks_list)):\r\n",
    "    num_blocks = num_blocks_list[i]\r\n",
    "    for j in range(num_blocks):\r\n",
    "        x = residual_block(x, downsample=(j==0 and i!=0), filters=num_filters)\r\n",
    "    num_filters *= 2\r\n",
    "    \r\n",
    "x, _ = attention(name = \"beat_att\")(x)\r\n",
    "\r\n",
    "##### Rhythm level\r\n",
    "x = K.reshape(x,(-1, int(sig_len/beat_size) , 64))\r\n",
    "\r\n",
    "x = Bidirectional(LSTM(32, return_sequences = True))(x)\r\n",
    "x, _ = attention(name = \"rhythm_att\")(x)\r\n",
    "\r\n",
    "\r\n",
    "#### Channel level\r\n",
    "\r\n",
    "x = K.reshape(x, (-1, num_channel, 64))\r\n",
    "x, _ = attention(name = \"channel_att\")(x)\r\n",
    "\r\n",
    "outputs = Dense(5, activation = 'sigmoid')(x)\r\n",
    "\r\n",
    "\r\n",
    "model = tf.keras.models.Model(inputs = inputs, outputs = outputs)\r\n",
    "\r\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy',tf.keras.metrics.AUC(multi_label = True)])\r\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3840"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('3_level_att_saves'):\n",
    "    os.mkdir('3_level_att_saves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project = '3_level_att', name = 'run_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accuracy metric\r\n",
    "\r\n",
    "def metrics(y_true, y_scores):\r\n",
    "    y_pred = y_scores >= 0.5\r\n",
    "    acc = np.zeros(y_pred.shape[-1])\r\n",
    "    \r\n",
    "    for i in range(y_pred.shape[-1]):\r\n",
    "        acc[i] = accuracy_score(y_true[:,i], y_pred[:,i])\r\n",
    "    return acc, np.mean(acc)\r\n",
    "\r\n",
    "## Callback for logging and metrics \r\n",
    "\r\n",
    "class model_checkpoint(tf.keras.callbacks.Callback):\r\n",
    "\r\n",
    "    def __init__(self, filepath, gen, monitor='loss',  options=None, **kwargs):\r\n",
    "\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.filepath = filepath\r\n",
    "        self.monitor = monitor\r\n",
    "        self.test_data = gen\r\n",
    "        \r\n",
    "        \r\n",
    "    def on_epoch_end(self, epoch, logs = {}) :\r\n",
    "        \r\n",
    "        test_len = len(self.test_data)\r\n",
    "        score = []\r\n",
    "        gt =[]\r\n",
    "\r\n",
    "        for i in range(test_len):\r\n",
    "            X,y = self.test_data[i][0], self.test_data[i][1]\r\n",
    "            temp_score = self.model.predict(X)\r\n",
    "            score.append(temp_score)\r\n",
    "            gt.append(y)\r\n",
    "\r\n",
    "        score = np.concatenate(score, axis = 0)\r\n",
    "        gt = np.concatenate(gt, axis = 0)\r\n",
    "        \r\n",
    "        roc_auc = roc_auc_score(gt, score, average = 'macro')\r\n",
    "        _, accuracy = metrics(gt, score)\r\n",
    "        \r\n",
    "        temp_path = f\"{epoch+1}_roc_{roc_auc:.4f}.h5\"\r\n",
    "        path = os.path.join(self.filepath, temp_path)\r\n",
    "        \r\n",
    "        if epoch > 5 :\r\n",
    "            self.model.save_weights(path)\r\n",
    "\r\n",
    "        wandb.log({'train_loss' : logs['loss'], 'epoch' : epoch})\r\n",
    "        wandb.log({'train_keras_auroc' : logs.get(self.monitor), 'epoch' : epoch})\r\n",
    "        \r\n",
    "        wandb.log({'test_loss' : logs['val_loss'], 'epoch' : epoch})\r\n",
    "        wandb.log({'test_keras_auroc' : logs['val_auc'], 'epoch' : epoch})\r\n",
    "\r\n",
    "        wandb.log({'test_roc_score' : roc_auc, 'epoch' : epoch})\r\n",
    "        wandb.log({'test_accuracy_score' : accuracy, 'epoch' : epoch})\r\n",
    "        \r\n",
    "        logs['val_roc_auc'] = roc_auc\r\n",
    "        logs['val_accuracy_score'] = accuracy\r\n",
    "    \r\n",
    "    def set_model(self, model):\r\n",
    "        self.model = model\r\n",
    "\r\n",
    "        \r\n",
    "metric = 'auc'\r\n",
    "checkpoint_filepath = '3_level_att_saves'\r\n",
    "\r\n",
    "checkpoint = model_checkpoint(checkpoint_filepath, monitor = metric, gen = test_gen )\r\n",
    "\r\n",
    "\r\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\r\n",
    "        factor=0.1,\r\n",
    "        patience=10,\r\n",
    "        min_lr=0.001 * 0.001)\r\n",
    "\r\n",
    "callbacks = [checkpoint, reduce_lr]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_gen, epochs = 60, validation_data = test_gen, workers = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_weights = r'49_roc_0.9216.h5'\r\n",
    "\r\n",
    "model.load_weights(path_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.921606040850872"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gen = DataGen(X_test_scale, y_test, batch_size = len(y_test))\n",
    "\n",
    "pred = model.predict(test_gen[0][0])\n",
    "\n",
    "roc_auc_score(y_test, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class wise accuracy: [0.88534443 0.91585761 0.87979658 0.87101248 0.89089228]\n",
      "accuracy: 0.888580674988442\n"
     ]
    }
   ],
   "source": [
    "### Accuracy metric\n",
    "\n",
    "def metrics(y_true, y_scores):\n",
    "    y_pred = y_scores >= 0.5\n",
    "    acc = np.zeros(y_pred.shape[-1])\n",
    "    \n",
    "    for i in range(y_pred.shape[-1]):\n",
    "        acc[i] = accuracy_score(y_true[:,i], y_pred[:,i])\n",
    "    return acc, np.mean(acc)\n",
    "\n",
    "acc, mean_acc = metrics(y_test, pred)\n",
    "print(f'class wise accuracy: {acc}')\n",
    "print(f'accuracy: {mean_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_score : 0.921606040850872\n",
      "class wise AUC : [0.91964736 0.88341405 0.92447519 0.94504497 0.93544863]\n"
     ]
    }
   ],
   "source": [
    "### Class wise AUC\n",
    "\n",
    "roc_score = roc_auc_score(y_test, pred, average='macro')\n",
    "print(f'roc_score : {roc_score}')\n",
    "\n",
    "def AUC(y_true: np.ndarray, y_pred: np.ndarray, verbose=False) -> float:\n",
    "    \"\"\"Computes the macro-average AUC score.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): list of labels\n",
    "        y_pred (np.ndarray): list of predicted probabilities\n",
    "\n",
    "    Returns:\n",
    "        float: macro-average AUC score.\n",
    "    \"\"\"\n",
    "    aucs = []\n",
    "    assert len(y_true.shape) == 2 and len(y_pred.shape) == 2, 'Predictions and labels must be 2D.'\n",
    "    for col in range(y_true.shape[1]):\n",
    "        try:\n",
    "            aucs.append(roc_auc_score(y_true[:, col], y_pred[:, col]))\n",
    "        except ValueError as e:\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f'Value error encountered for label {col}, likely due to using mixup or '\n",
    "                    f'lack of full label presence. Setting AUC to accuracy. '\n",
    "                    f'Original error was: {str(e)}.'\n",
    "                )\n",
    "            aucs.append((y_pred == y_true).sum() / len(y_pred))\n",
    "    return np.array(aucs)\n",
    "\n",
    "class_auc = AUC(y_test, pred)\n",
    "print(f'class wise AUC : {class_auc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.82      0.64      0.72       498\n",
      "         HYP       0.78      0.43      0.56       263\n",
      "          MI       0.83      0.67      0.74       553\n",
      "        NORM       0.83      0.90      0.86       964\n",
      "        STTC       0.81      0.72      0.76       523\n",
      "\n",
      "   micro avg       0.82      0.73      0.77      2801\n",
      "   macro avg       0.81      0.67      0.73      2801\n",
      "weighted avg       0.82      0.73      0.76      2801\n",
      " samples avg       0.79      0.76      0.76      2801\n",
      "\n",
      "C:\\Users\\likit\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_values = pred >= 0.5\n",
    "\n",
    "report = classification_report(y_test, pred_values, target_names = mlb.classes_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8057840187406535,\n",
       " 0.921606040850872,\n",
       " array([0.41142773, 0.76653767, 0.80528875, 0.80578402, 0.79801804,\n",
       "        0.78467242, 0.75753944, 0.72340467, 0.64063727,        nan]),\n",
       " array([0.25899214, 0.65308984, 0.74734166, 0.78767813, 0.81797619,\n",
       "        0.84193254, 0.85984238, 0.89507299, 0.92151608,        nan]),\n",
       " array([1.        , 0.92768531, 0.87297735, 0.82474187, 0.77901063,\n",
       "        0.73470489, 0.67699183, 0.60698875, 0.49098474, 0.        ]),\n",
       " array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "        0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multi_threshold_precision_recall(y_true: np.ndarray, y_pred: np.ndarray, thresholds: np.ndarray) :\n",
    "    \n",
    "    # Expand analysis to number of thresholds\n",
    "    y_pred_bin = np.repeat(y_pred[None, :, :], len(thresholds), axis=0) >= thresholds[:, None, None]\n",
    "    y_true_bin = np.repeat(y_true[None, :, :], len(thresholds), axis=0)\n",
    "\n",
    "    # Compute true positives\n",
    "    TP = np.sum(np.logical_and(y_true, y_pred_bin), axis=2)\n",
    "\n",
    "    # Compute macro-average precision handling all warnings\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        den = np.sum(y_pred_bin, axis=2)\n",
    "        precision = TP / den\n",
    "        precision[den == 0] = np.nan\n",
    "        with warnings.catch_warnings():  # for nan slices\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            av_precision = np.nanmean(precision, axis=1)\n",
    "\n",
    "    # Compute macro-average recall\n",
    "    recall = TP / np.sum(y_true_bin, axis=2)\n",
    "    av_recall = np.mean(recall, axis=1)\n",
    "\n",
    "    return av_precision, av_recall\n",
    "\n",
    "\n",
    "def metric_summary(y_true: np.ndarray, y_pred: np.ndarray, num_thresholds: int = 10) :\n",
    "    \n",
    "    thresholds = np.arange(0.00, 1.01, 1. / (num_thresholds - 1), float)\n",
    "    average_precisions, average_recalls = multi_threshold_precision_recall(\n",
    "        y_true, y_pred, thresholds\n",
    "    )\n",
    "    f_scores = 2 * (average_precisions * average_recalls) / (average_precisions + average_recalls)\n",
    "    auc = np.array(AUC(y_true, y_pred, verbose=True)).mean()\n",
    "    return (\n",
    "        f_scores[np.nanargmax(f_scores)],\n",
    "        auc,\n",
    "        f_scores,\n",
    "        average_precisions,\n",
    "        average_recalls,\n",
    "        thresholds\n",
    "    )\n",
    "\n",
    "metric_summary(y_test, pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tf': conda)",
   "name": "python37764bittfgpucondad1048e2c1cb249e3861f34c4a36205b2"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "8ec611c835db78d84b6792c6109a40fde2d82a4b51b510d36b5492580aff59ca"
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": "50",
    "lenVar": "100"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}